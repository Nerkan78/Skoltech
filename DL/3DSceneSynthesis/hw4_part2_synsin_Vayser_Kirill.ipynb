{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "synsin_Vayser_Kirill.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/20357655/145710301-ad00ab66-2378-404f-a918-576aba834ff9.png)"
      ],
      "metadata": {
        "id": "8ccbda8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will try to implement a `View Synthesis` model that allows us to generate new scene views based on a single image.\n",
        "\n",
        "The basic idea is to use `differentiable point cloud rendering`, which is used to convert a hidden 3D feature point cloud into a target view.\n",
        "The projected features are decoded by `refinement network` to inpaint missing regions and generate a realistic output image."
      ],
      "metadata": {
        "id": "2d8c8e89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall pipeline disribed below"
      ],
      "metadata": {
        "id": "083c52ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/20357655/145710444-0d0e163f-6996-4eb8-81c0-69798b11c5a6.png)"
      ],
      "metadata": {
        "id": "b90c6e49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "## Download KITTI dataset"
      ],
      "metadata": {
        "id": "9712addf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework04/gfile.py\n",
        "! wget https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall21/homework04/kitti.py"
      ],
      "metadata": {
        "id": "26e083a7",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:01.120994Z",
          "iopub.execute_input": "2022-05-13T19:10:01.121273Z",
          "iopub.status.idle": "2022-05-13T19:10:03.672339Z",
          "shell.execute_reply.started": "2022-05-13T19:10:01.121243Z",
          "shell.execute_reply": "2022-05-13T19:10:03.671460Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ``` python \n",
        " from gfile import download_file_from_google_drive\n",
        " \n",
        " download_file_from_google_drive(\n",
        "    '1lqspXN10biBShBIVD0yvgnl1nIPPhRdC',\n",
        "     'kitti.zip'\n",
        " )\n",
        "```\n",
        "Optionally you can download with native gdown\n",
        "\n",
        "``` sh\n",
        "    gdown --id 1lqspXN10biBShBIVD0yvgnl1nIPPhRdC\n",
        "```"
      ],
      "metadata": {
        "id": "4bde8b88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from gfile import download_file_from_google_drive\n",
        "\n",
        " download_file_from_google_drive(\n",
        "    '1lqspXN10biBShBIVD0yvgnl1nIPPhRdC',\n",
        "     'kitti.zip'\n",
        " )"
      ],
      "metadata": {
        "id": "C5hQpwvJyVic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown 1lqspXN10biBShBIVD0yvgnl1nIPPhRdC"
      ],
      "metadata": {
        "id": "ecXZNe6feFkY",
        "execution": {
          "iopub.status.busy": "2022-05-13T18:50:39.252123Z",
          "iopub.execute_input": "2022-05-13T18:50:39.252683Z",
          "iopub.status.idle": "2022-05-13T18:51:22.375930Z",
          "shell.execute_reply.started": "2022-05-13T18:50:39.252602Z",
          "shell.execute_reply": "2022-05-13T18:51:22.374598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip dataset_kitti.zip"
      ],
      "metadata": {
        "id": "1b12756c",
        "execution": {
          "iopub.status.busy": "2022-05-13T18:51:39.342880Z",
          "iopub.execute_input": "2022-05-13T18:51:39.343668Z",
          "iopub.status.idle": "2022-05-13T18:52:03.256829Z",
          "shell.execute_reply.started": "2022-05-13T18:51:39.343625Z",
          "shell.execute_reply": "2022-05-13T18:52:03.256050Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pytorch3d installation\n",
        "\n",
        "The pytorch3d library needed to work with geometry. We highly encourage to see on the https://pytorch3d.org/tutorials/\n",
        "\n",
        "To give you hints we provide some snipets, that can work at colab (deends from the overal environment).\n",
        "\n"
      ],
      "metadata": {
        "id": "4eded662"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you version is low enough, you will not have problems\n",
        "# optionally you can downgrade pytorch and use native installation with the following line\n",
        "import torch\n",
        "if torch.__version__.startswith(\"1.11.\"):\n",
        "  !pip3 install torch==1.7.1+cu102  torchvision==0.8.2+cu102  -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "id": "9a46ca38",
        "execution": {
          "iopub.status.busy": "2022-05-13T18:52:52.040638Z",
          "iopub.execute_input": "2022-05-13T18:52:52.040913Z",
          "iopub.status.idle": "2022-05-13T18:52:53.759152Z",
          "shell.execute_reply.started": "2022-05-13T18:52:52.040882Z",
          "shell.execute_reply": "2022-05-13T18:52:53.758225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "6V69fqgI9f0A",
        "execution": {
          "iopub.status.busy": "2022-05-13T18:52:53.760730Z",
          "iopub.execute_input": "2022-05-13T18:52:53.761251Z",
          "iopub.status.idle": "2022-05-13T18:52:53.772821Z",
          "shell.execute_reply.started": "2022-05-13T18:52:53.761219Z",
          "shell.execute_reply": "2022-05-13T18:52:53.771947Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"1.10.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
        "        !tar xzf 1.10.0.tar.gz\n",
        "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "metadata": {
        "id": "d04040f2",
        "execution": {
          "iopub.status.busy": "2022-05-13T18:52:55.908941Z",
          "iopub.execute_input": "2022-05-13T18:52:55.909211Z",
          "iopub.status.idle": "2022-05-13T19:09:06.122891Z",
          "shell.execute_reply.started": "2022-05-13T18:52:55.909163Z",
          "shell.execute_reply": "2022-05-13T19:09:06.122015Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "0de1f7dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "from IPython.display import clear_output, HTML\n",
        "from collections import defaultdict\n",
        "\n",
        "from kitti import KITTIDataLoader\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "\n",
        "from pytorch3d.vis.plotly_vis import plot_scene\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.renderer import PerspectiveCameras, compositing, rasterize_points"
      ],
      "metadata": {
        "id": "a7d09277",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:12.912479Z",
          "iopub.execute_input": "2022-05-13T19:10:12.912748Z",
          "iopub.status.idle": "2022-05-13T19:10:13.402371Z",
          "shell.execute_reply.started": "2022-05-13T19:10:12.912718Z",
          "shell.execute_reply": "2022-05-13T19:10:13.401611Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_RT(RT):\n",
        "    return RT[..., :3, :3], RT[..., :3, 3]\n",
        "\n",
        "def renormalize_image(image):\n",
        "    return image * 0.5 + 0.5"
      ],
      "metadata": {
        "id": "1a6b9b1f",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:17.015056Z",
          "iopub.execute_input": "2022-05-13T19:10:17.015991Z",
          "iopub.status.idle": "2022-05-13T19:10:17.022786Z",
          "shell.execute_reply.started": "2022-05-13T19:10:17.015938Z",
          "shell.execute_reply": "2022-05-13T19:10:17.021850Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = KITTIDataLoader('dataset_kitti')"
      ],
      "metadata": {
        "id": "f7cc1f15",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:17.450698Z",
          "iopub.execute_input": "2022-05-13T19:10:17.451201Z",
          "iopub.status.idle": "2022-05-13T19:10:17.692665Z",
          "shell.execute_reply.started": "2022-05-13T19:10:17.451145Z",
          "shell.execute_reply": "2022-05-13T19:10:17.691827Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each instance of dataset contain `source` and `target` images, `extrinsic` and `intrinsic` camera parameters for `source` and `targer` images.\n",
        "\n",
        "It is highly recommended to understand these concepts, e.g., here https://ksimek.github.io/2012/08/22/extrinsic/"
      ],
      "metadata": {
        "id": "a536bbe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, cameras = dataset[0].values()"
      ],
      "metadata": {
        "id": "ccd27eca",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:18.348880Z",
          "iopub.execute_input": "2022-05-13T19:10:18.349919Z",
          "iopub.status.idle": "2022-05-13T19:10:18.396401Z",
          "shell.execute_reply.started": "2022-05-13T19:10:18.349854Z",
          "shell.execute_reply": "2022-05-13T19:10:18.395444Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "ax = plt.subplot(1, 2, 1)\n",
        "ax.imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)\n",
        "ax.set_title('Source Image Frame', fontsize=20)\n",
        "ax.axis('off')\n",
        "\n",
        "ax = plt.subplot(1, 2, 2)\n",
        "ax.imshow(images[1].permute(1, 2, 0) * 0.5 + 0.5)\n",
        "ax.set_title('Target Image Frame', fontsize=20)\n",
        "ax.axis('off')"
      ],
      "metadata": {
        "id": "47657b15",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:19.574943Z",
          "iopub.execute_input": "2022-05-13T19:10:19.575222Z",
          "iopub.status.idle": "2022-05-13T19:10:20.260499Z",
          "shell.execute_reply.started": "2022-05-13T19:10:19.575171Z",
          "shell.execute_reply": "2022-05-13T19:10:20.259848Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_camera = PerspectiveCameras(\n",
        "    R=split_RT(cameras[0]['P'])[0][None],\n",
        "    T=split_RT(cameras[0]['P'])[1][None],\n",
        "    K=torch.from_numpy(cameras[0]['K'])[None]\n",
        ")\n",
        "\n",
        "target_camera = PerspectiveCameras(\n",
        "    R=split_RT(cameras[1]['P'])[0][None],\n",
        "    T=split_RT(cameras[1]['P'])[1][None],\n",
        "    K=torch.from_numpy(cameras[1]['K'])[None]\n",
        ")\n",
        "\n",
        "plot_scene(\n",
        "    {\n",
        "        'scene': {\n",
        "            'source_camera': source_camera,\n",
        "            'target_camera': target_camera\n",
        "        }\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "83fdc36a",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:20.261683Z",
          "iopub.execute_input": "2022-05-13T19:10:20.262380Z",
          "iopub.status.idle": "2022-05-13T19:10:20.755222Z",
          "shell.execute_reply.started": "2022-05-13T19:10:20.262330Z",
          "shell.execute_reply": "2022-05-13T19:10:20.754543Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "\n",
        "train_indexes = indexes[:-1000]\n",
        "validation_indexes = indexes[-1000:]\n",
        "\n",
        "train_dataset = Subset(dataset, train_indexes)\n",
        "validation_dataset = Subset(dataset, validation_indexes)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=16, num_workers=2,\n",
        "    shuffle=True, drop_last=True, pin_memory=True\n",
        ")\n",
        "validation_dataloder = DataLoader(\n",
        "    validation_dataset, batch_size=10, num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "ffb87ce0",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:20.775512Z",
          "iopub.execute_input": "2022-05-13T19:10:20.775711Z",
          "iopub.status.idle": "2022-05-13T19:10:20.784778Z",
          "shell.execute_reply.started": "2022-05-13T19:10:20.775687Z",
          "shell.execute_reply": "2022-05-13T19:10:20.783922Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "a1463727"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "Our main task here is to understand the simple way to solve novel view synthesis problem from single image.\n",
        "We need to implement `Spatial Feature Predictor`, `Depth Regressor`, `Point Cloud Renderer` and `RefinementNetwork`."
      ],
      "metadata": {
        "id": "a8b2df13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the main building blocks in these networks is `ResNetBlock`, but with some modifications:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711537-eebb0cb9-8935-4d65-bc4b-559c1e19ba98.png)\n",
        "\n",
        "So, let's implement it, but without the noise part `Linear + z`"
      ],
      "metadata": {
        "id": "5e26dacb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int = 1,\n",
        "        mode = 'identity'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.activation = nn.LeakyReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.identity_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        \n",
        "        if mode == 'identity':\n",
        "            self.identity_upsample = nn.Identity()\n",
        "        elif mode == 'up':\n",
        "            self.identity_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        elif mode == 'down':\n",
        "            self.identity_upsample = nn.AvgPool2d(kernel_size=2)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        identity = input\n",
        "\n",
        "        out = self.bn1(input)\n",
        "        out = self.activation(out)\n",
        "        out = self.identity_upsample(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        identity = self.identity_upsample(identity)\n",
        "        identity = self.identity_conv(identity)\n",
        "        out += identity\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "d6aa09af",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:32.839365Z",
          "iopub.execute_input": "2022-05-13T19:10:32.839924Z",
          "iopub.status.idle": "2022-05-13T19:10:32.859388Z",
          "shell.execute_reply.started": "2022-05-13T19:10:32.839879Z",
          "shell.execute_reply": "2022-05-13T19:10:32.858451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spatial Feature Predictor\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711931-be08e4f9-f383-4942-8b93-f8bdfd3060d2.png)"
      ],
      "metadata": {
        "id": "44866d81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialFeatureNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=64):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            ResNetBlock(3, 16),\n",
        "            ResNetBlock(16, 16),\n",
        "            ResNetBlock(16, 16),\n",
        "            ResNetBlock(16, 32),\n",
        "            ResNetBlock(32, 32),\n",
        "            ResNetBlock(32, 32),\n",
        "            ResNetBlock(32, 32),\n",
        "            ResNetBlock(32, 64),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return self.blocks(input)\n",
        "\n",
        "sf_net = SpatialFeatureNetwork()\n",
        "\n",
        "test_in = torch.rand(2, 3, 100, 100) \n",
        "test_out = sf_net(test_in)\n",
        "assert list(test_out.shape) == [2, 64, 100, 100], test_out.shape\n",
        "assert len(sf_net.blocks) == 8, len(sf_net.blocks)"
      ],
      "metadata": {
        "id": "e76a42f6",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:33.484414Z",
          "iopub.execute_input": "2022-05-13T19:10:33.485129Z",
          "iopub.status.idle": "2022-05-13T19:10:33.737203Z",
          "shell.execute_reply.started": "2022-05-13T19:10:33.485090Z",
          "shell.execute_reply": "2022-05-13T19:10:33.736428Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depth Regressor\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711785-690008e5-96d0-418f-adf1-1509e399c92e.png)\n",
        "\n",
        "A green convolution has parameters: stride 2, padding 1, kernel size 4.\n",
        "\n",
        "An `Enc Block` consists of a sequence of Leaky ReLU, convolution (stride 2, padding 1, kernel size 4), and batch normalisation layers.\n",
        "\n",
        "A `Dec Block` consists of a sequence of ReLU, 2x bilinear upsampling, convolution (stride 1, padding 1, kernel size3), and batch normalisation layers (except for the final layer, which has no batch normalisation layer)."
      ],
      "metadata": {
        "id": "622d6c5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "\n",
        "# TODO class EncoderBlock\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "test_input = torch.randn(2, 64, 128, 128)\n",
        "encoder = EncoderBlock(64, 128)\n",
        "assert list(encoder(test_input).shape) == [2, 128, 64, 64], encoder(test_input).shape\n",
        "\n",
        "# TODO class DecoderBlock\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "decoder = DecoderBlock(64, 32)\n",
        "assert list(decoder(test_input).shape) == [2, 32, 256, 256], decoder(test_input).shape\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters=32,\n",
        "        channels_in=3,\n",
        "        channels_out=3\n",
        "    ):\n",
        "        super(Unet, self).__init__()\n",
        "        self.encoder = nn.ModuleList([\n",
        "            nn.Conv2d(channels_in, num_filters, 4, 2, 1),\n",
        "            EncoderBlock(num_filters, num_filters * 2),\n",
        "            EncoderBlock(num_filters * 2, num_filters * 4),\n",
        "            EncoderBlock(num_filters * 4, num_filters * 8),\n",
        "            EncoderBlock(num_filters * 8, num_filters * 8),\n",
        "            EncoderBlock(num_filters * 8, num_filters * 8),\n",
        "            EncoderBlock(num_filters * 8, num_filters * 8),\n",
        "            EncoderBlock(num_filters * 8, num_filters * 8)\n",
        "\n",
        "        ])\n",
        "        self.decoder = nn.ModuleList([\n",
        "            DecoderBlock(num_filters * 8, num_filters * 8),\n",
        "            DecoderBlock(num_filters * 8, num_filters * 8),\n",
        "            DecoderBlock(num_filters * 8, num_filters * 8),\n",
        "            DecoderBlock(num_filters * 8, num_filters * 8),\n",
        "            DecoderBlock(num_filters * 8, num_filters * 4),\n",
        "            DecoderBlock(num_filters * 4, num_filters * 2),\n",
        "            DecoderBlock(num_filters * 2, num_filters)            \n",
        "        ])\n",
        "        self.final_layer = DecoderBlock(num_filters, channels_out, batch_norm=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, input):\n",
        "      # TODO  x in [-1, 1]\n",
        "        encoded_outputs = []\n",
        "        x = input\n",
        "        for i, enc_block in enumerate(self.encoder):\n",
        "            x = enc_block(x)\n",
        "            if i < len(self.encoder) + 1: \n",
        "                encoded_outputs.append(x)\n",
        "        for dec_block, encoded_output in zip(self.decoder, encoded_outputs[::-1]):\n",
        "            x = dec_block(encoded_output + x)\n",
        "        x = self.final_layer(x)\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "unet = Unet()\n",
        "test_input = torch.randn(2, 3, 512, 512)\n",
        "assert unet(test_input).shape == test_input.shape, unet(test_input).shape"
      ],
      "metadata": {
        "id": "1cd12e4c",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:33.860101Z",
          "iopub.execute_input": "2022-05-13T19:10:33.860645Z",
          "iopub.status.idle": "2022-05-13T19:10:34.787908Z",
          "shell.execute_reply.started": "2022-05-13T19:10:33.860609Z",
          "shell.execute_reply": "2022-05-13T19:10:34.787130Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refinement Network\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145711921-45ebf1e5-e852-4c47-8b93-d545f67dc6bf.png)"
      ],
      "metadata": {
        "id": "ea7025ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RefinementNetwork(nn.Module):\n",
        "    def __init__(self, in_channels=64, out_channels=3):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(\n",
        "            ResNetBlock(in_channels, 32),\n",
        "            ResNetBlock(32, 128, mode='down'),\n",
        "            ResNetBlock(128, 128, mode='down'),\n",
        "            ResNetBlock(128, 64),\n",
        "            ResNetBlock(64, 64, mode='up'),\n",
        "            ResNetBlock(64, 64, mode='up'),\n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, out_channels),\n",
        "        )\n",
        "    \n",
        "    def forward(self, input: torch.Tensor):\n",
        "        return self.blocks(input)\n",
        "\n",
        "test_input = torch.randn(2, 64, 128, 128)\n",
        "rn = RefinementNetwork()\n",
        "assert list(rn(test_input).shape) == [2, 3, 128, 128], rn(test_input).shape"
      ],
      "metadata": {
        "id": "3f40b316",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:34.789607Z",
          "iopub.execute_input": "2022-05-13T19:10:34.790022Z",
          "iopub.status.idle": "2022-05-13T19:10:35.151835Z",
          "shell.execute_reply.started": "2022-05-13T19:10:34.789979Z",
          "shell.execute_reply": "2022-05-13T19:10:35.151053Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary network"
      ],
      "metadata": {
        "id": "4061ce38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG19(nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = torchvision.models.vgg19(\n",
        "            pretrained=True\n",
        "        ).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Normalize the image so that it is in the appropriate range\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out"
      ],
      "metadata": {
        "id": "3dc2b81e",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:35.406613Z",
          "iopub.execute_input": "2022-05-13T19:10:35.407483Z",
          "iopub.status.idle": "2022-05-13T19:10:35.418061Z",
          "shell.execute_reply.started": "2022-05-13T19:10:35.407438Z",
          "shell.execute_reply": "2022-05-13T19:10:35.417237Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "7ecc0036"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criterions & Metrics"
      ],
      "metadata": {
        "id": "fb955b50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Set to false so that this part of the network is frozen\n",
        "        self.model = VGG19(requires_grad=False)\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
        "\n",
        "    def forward(self, pred_img, gt_img):\n",
        "        gt_fs = self.model(gt_img)\n",
        "        pred_fs = self.model(pred_img)\n",
        "\n",
        "        # Collect the losses at multiple layers (need unsqueeze in\n",
        "        # order to concatenate these together)\n",
        "        loss = 0\n",
        "        for i in range(0, len(gt_fs)):\n",
        "            loss += self.weights[i] * self.criterion(pred_fs[i], gt_fs[i])\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "ad4a8159",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:36.471673Z",
          "iopub.execute_input": "2022-05-13T19:10:36.471936Z",
          "iopub.status.idle": "2022-05-13T19:10:36.479207Z",
          "shell.execute_reply.started": "2022-05-13T19:10:36.471906Z",
          "shell.execute_reply": "2022-05-13T19:10:36.478334Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def psnr(predicted_image, target_image):\n",
        "    batch_size = predicted_image.size(0)\n",
        "    mse_err = (\n",
        "        (predicted_image - target_image)\n",
        "        .pow(2).sum(dim=1)\n",
        "        .view(batch_size, -1).mean(dim=1)\n",
        "    )\n",
        "\n",
        "    psnr = 10 * (1 / mse_err).log10()\n",
        "    return psnr.mean()"
      ],
      "metadata": {
        "id": "81835854",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:37.381053Z",
          "iopub.execute_input": "2022-05-13T19:10:37.381738Z",
          "iopub.status.idle": "2022-05-13T19:10:37.386277Z",
          "shell.execute_reply.started": "2022-05-13T19:10:37.381699Z",
          "shell.execute_reply": "2022-05-13T19:10:37.385588Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "d3e7d6f0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Point Cloud Renderer\n",
        "\n",
        "`Differential Rasterization` is a key component of our system. We will use the algorithm already implemented in `pytorch3d`.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/20357655/145715968-94abbe1a-8d14-4c20-98c4-61afd9161ada.png)\n",
        "\n",
        "For more details read (3.2) https://arxiv.org/pdf/1912.08804.pdf"
      ],
      "metadata": {
        "id": "553d5034"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointsRasterizerWithBlending(nn.Module):\n",
        "    \"\"\"\n",
        "    Rasterizes a set of points using a differentiable renderer. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, radius=1.5, image_size=256, points_per_pixel=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.radius = radius\n",
        "        self.image_size = image_size\n",
        "        self.points_per_pixel = points_per_pixel\n",
        "        \n",
        "        self.rad_pow = 2\n",
        "        self.tau = 1.0\n",
        "\n",
        "    def forward(self, point_cloud, spatial_features):\n",
        "        batch_size = spatial_features.size(0)\n",
        "\n",
        "        # Make sure these have been arranged in the same way\n",
        "        assert point_cloud.size(2) == 3\n",
        "        assert point_cloud.size(1) == spatial_features.size(2)\n",
        "\n",
        "        point_cloud[:, :, 1] = -point_cloud[:, :, 1]\n",
        "        point_cloud[:, :, 0] = -point_cloud[:, :, 0]\n",
        "\n",
        "        radius = float(self.radius) / float(self.image_size) * 2.0\n",
        "\n",
        "        point_cloud = Pointclouds(points=point_cloud, features=spatial_features.permute(0, 2, 1))\n",
        "        points_idx, _, dist = rasterize_points(\n",
        "            point_cloud, self.image_size, radius, self.points_per_pixel\n",
        "        )\n",
        "\n",
        "        dist = dist / pow(radius, self.rad_pow)\n",
        "\n",
        "        alphas = (\n",
        "            (1 - dist.clamp(max=1, min=1e-3).pow(0.5))\n",
        "            .pow(self.tau)\n",
        "            .permute(0, 3, 1, 2)\n",
        "        )\n",
        "    \n",
        "        transformed_src_alphas = compositing.alpha_composite(\n",
        "            points_idx.permute(0, 3, 1, 2).long(),\n",
        "            alphas,\n",
        "            point_cloud.features_packed().permute(1, 0),\n",
        "        )\n",
        "\n",
        "        return transformed_src_alphas"
      ],
      "metadata": {
        "id": "73f59d89",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:39.235313Z",
          "iopub.execute_input": "2022-05-13T19:10:39.235979Z",
          "iopub.status.idle": "2022-05-13T19:10:39.249716Z",
          "shell.execute_reply.started": "2022-05-13T19:10:39.235940Z",
          "shell.execute_reply": "2022-05-13T19:10:39.248882Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And `PointsManipulator` do the following steps:\n",
        "\n",
        "    1) Create virtual image place in [normalized coordinate](https://pytorch3d.org/docs/cameras)\n",
        "    2) Move camera according to `regressed depth`\n",
        "    3) Rotate points according to target camera paramers\n",
        "    4) And finally render them with help of `PointsRasterizerWithBlending`"
      ],
      "metadata": {
        "id": "b8303a60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointsManipulator(nn.Module):\n",
        "    EPS = 1e-5\n",
        "\n",
        "    def __init__(self, image_size):\n",
        "        super().__init__()\n",
        "        # Assume that image plane is square\n",
        "\n",
        "        self.splatter = PointsRasterizerWithBlending(\n",
        "            radius=1.0,\n",
        "            image_size=image_size,\n",
        "            points_per_pixel=128,\n",
        "        )\n",
        "\n",
        "        xs = torch.linspace(0, image_size - 1, image_size) / \\\n",
        "            float(image_size - 1) * 2 - 1\n",
        "        ys = torch.linspace(0, image_size - 1, image_size) / \\\n",
        "            float(image_size - 1) * 2 - 1\n",
        "\n",
        "        xs = xs.view(1, 1, 1, image_size).repeat(1, 1, image_size, 1)\n",
        "        ys = ys.view(1, 1, image_size, 1).repeat(1, 1, 1, image_size)\n",
        "\n",
        "        xyzs = torch.cat(\n",
        "            (xs, -ys, -torch.ones(xs.size()), torch.ones(xs.size())), 1\n",
        "        ).view(1, 4, -1)\n",
        "\n",
        "        self.register_buffer(\"xyzs\", xyzs)\n",
        "\n",
        "    def project_pts(self, depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2):\n",
        "        # Project the world points into the new view\n",
        "        projected_coors = self.xyzs * depth\n",
        "        projected_coors[:, -1, :] = 1\n",
        "\n",
        "        # Transform into camera coordinate of the first view\n",
        "        cam1_X = K_inv.bmm(projected_coors)\n",
        "\n",
        "        # Transform into world coordinates\n",
        "        RT = RT_cam2.bmm(RTinv_cam1)\n",
        "\n",
        "        wrld_X = RT.bmm(cam1_X)\n",
        "\n",
        "        # And intrinsics\n",
        "        xy_proj = K.bmm(wrld_X)\n",
        "\n",
        "        # And finally we project to get the final result\n",
        "        mask = (xy_proj[:, 2:3, :].abs() < self.EPS).detach()\n",
        "\n",
        "        # Remove invalid zs that cause nans\n",
        "        zs = xy_proj[:, 2:3, :]\n",
        "        zs[mask] = self.EPS\n",
        "\n",
        "        sampler = torch.cat((xy_proj[:, 0:2, :] / -zs, xy_proj[:, 2:3, :]), 1)\n",
        "        sampler[mask.repeat(1, 3, 1)] = -10\n",
        "        # Flip the ys\n",
        "        sampler = sampler * torch.Tensor([1, -1, -1]).unsqueeze(0).unsqueeze(\n",
        "            2\n",
        "        ).to(sampler.device)\n",
        "\n",
        "        return sampler\n",
        "\n",
        "    def forward_justpts(\n",
        "        self,\n",
        "        spatial_features, depth,\n",
        "        K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "    ):\n",
        "        # Now project these points into a new view\n",
        "        batch_size, c, w, h = spatial_features.size()\n",
        "\n",
        "        if len(depth.size()) > 3:\n",
        "            # reshape into the right positioning\n",
        "            depth = depth.view(batch_size, 1, -1)\n",
        "            spatial_features = spatial_features.view(batch_size, c, -1)\n",
        "\n",
        "        pointcloud = self.project_pts(\n",
        "            depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "        )\n",
        "        pointcloud = pointcloud.permute(0, 2, 1).contiguous()\n",
        "        result = self.splatter(pointcloud, spatial_features)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "10446563",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:42.402902Z",
          "iopub.execute_input": "2022-05-13T19:10:42.403474Z",
          "iopub.status.idle": "2022-05-13T19:10:42.419111Z",
          "shell.execute_reply.started": "2022-05-13T19:10:42.403433Z",
          "shell.execute_reply": "2022-05-13T19:10:42.418415Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "0641cd48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All together"
      ],
      "metadata": {
        "id": "0e5ab269"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewSynthesisModel(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.spatial_feature_predictor = SpatialFeatureNetwork()\n",
        "        self.depth_regressor = Unet(channels_in=3, channels_out=1)\n",
        "        self.point_cloud_renderer = PointsManipulator(image_size=256)\n",
        "        self.refinement_network = RefinementNetwork()\n",
        "\n",
        "        # Special constant for KITTI dataset\n",
        "        self.z_min = 1.0\n",
        "        self.z_max = 50.0\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_image,\n",
        "        K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "        #TODO\n",
        "    ):\n",
        "        # TODO\n",
        "        # 1) Predict spatial feature for source image\n",
        "        # 2) Predict depth for source image (dont forget to renormalize depth with z_min/z_max)\n",
        "        # 3) Generate new features with `point_cloud_renderer`\n",
        "        # 4) And finnaly apply `refinement_network` to obtain new image\n",
        "        # 5) return new image, and depth of source image\n",
        "\n",
        "        features = self.spatial_feature_predictor(input_image)\n",
        "        regressed_depth = (self.depth_regressor(input_image)  + 1) * (self.z_max - self.z_min) / 2 + self.z_min\n",
        "        points = self.point_cloud_renderer.forward_justpts(features, regressed_depth, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2)\n",
        "        generated_image = self.refinement_network(points)\n",
        "        return generated_image, regressed_depth"
      ],
      "metadata": {
        "id": "ae8be184",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:10:46.907902Z",
          "iopub.execute_input": "2022-05-13T19:10:46.908436Z",
          "iopub.status.idle": "2022-05-13T19:10:46.916505Z",
          "shell.execute_reply.started": "2022-05-13T19:10:46.908380Z",
          "shell.execute_reply": "2022-05-13T19:10:46.915605Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3978e549"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "In order for the work to be accepted, you must achieve a quality of ~0.5 (validation loss value) and visualize several samples as in the example"
      ],
      "metadata": {
        "id": "3b553c72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "model = ViewSynthesisModel().to(device)\n",
        "model.load_state_dict(torch.load('../input/SynSin-checkpoint/synsin_checkpoint_100_epoch.pth'))\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
        "histoty = defaultdict(list)\n",
        "\n",
        "l1_criterion = nn.L1Loss()\n",
        "perceptual_criterion = PerceptualLoss().to(device)"
      ],
      "metadata": {
        "id": "cc7f94ea",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:11:38.042125Z",
          "iopub.execute_input": "2022-05-13T19:11:38.042890Z",
          "iopub.status.idle": "2022-05-13T19:12:33.149261Z",
          "shell.execute_reply.started": "2022-05-13T19:11:38.042851Z",
          "shell.execute_reply": "2022-05-13T19:12:33.148452Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "\n",
        "train_indexes = indexes[:1000]\n",
        "validation_indexes = indexes[:300]\n",
        "\n",
        "train_dataset = Subset(dataset, train_indexes)\n",
        "validation_dataset = Subset(dataset, validation_indexes)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "num_epochs = 1 #100\n",
        "val_batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=2,\n",
        "    shuffle=True, drop_last=True, pin_memory=True\n",
        ")\n",
        "validation_dataloder = DataLoader(\n",
        "    validation_dataset, batch_size=val_batch_size, num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "45f5a34e",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:13:06.289506Z",
          "iopub.execute_input": "2022-05-13T19:13:06.289769Z",
          "iopub.status.idle": "2022-05-13T19:13:06.296377Z",
          "shell.execute_reply.started": "2022-05-13T19:13:06.289739Z",
          "shell.execute_reply": "2022-05-13T19:13:06.295647Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, batch in tqdm(enumerate(train_dataloader, 2), total=len(train_dataloader)):\n",
        "        source_image = batch[\"images\"][0].to(device)\n",
        "        target_image = batch[\"images\"][-1].to(device)\n",
        "\n",
        "        # TODO\n",
        "        RT_cam1 = batch[\"cameras\"][0]['P'].to(device)\n",
        "        RTinv_cam1 = batch[\"cameras\"][0]['Pinv'].to(device)\n",
        "\n",
        "        K = batch['cameras'][0]['K'].to(device)\n",
        "        K_inv = batch['cameras'][0]['Kinv'].to(device)\n",
        "\n",
        "        RT_cam2 = batch[\"cameras\"][-1]['P'].to(device)\n",
        "        RTinv_cam2 = batch[\"cameras\"][-1]['Pinv'].to(device)\n",
        "\n",
        "\n",
        "        generated_image, regressed_depth = model(source_image, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2 )\n",
        "\n",
        "        loss = l1_criterion(generated_image, target_image) \\\n",
        "            + 10 * perceptual_criterion(\n",
        "            renormalize_image(generated_image),\n",
        "            renormalize_image(target_image)\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        histoty['train_loss'].append(loss.item())\n",
        "    \n",
        "    for i, batch in tqdm(enumerate(validation_dataloder), total=len(validation_dataloder)):\n",
        "        source_image = batch[\"images\"][0].to(device)\n",
        "        target_image = batch[\"images\"][-1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO\n",
        "            RT_cam1 = batch[\"cameras\"][0]['P'].to(device)\n",
        "            RTinv_cam1 = batch[\"cameras\"][0]['Pinv'].to(device)\n",
        "\n",
        "            K = batch['cameras'][0]['K'].to(device)\n",
        "            K_inv = batch['cameras'][0]['Kinv'].to(device)\n",
        "\n",
        "            RT_cam2 = batch[\"cameras\"][-1]['P'].to(device)\n",
        "            RTinv_cam2 = batch[\"cameras\"][-1]['Pinv'].to(device)\n",
        "            generated_image, regressed_depth = model(source_image, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2 )\n",
        "\n",
        "        loss = l1_criterion(generated_image, target_image) \\\n",
        "            + 10 * perceptual_criterion(\n",
        "            renormalize_image(generated_image),\n",
        "            renormalize_image(target_image)\n",
        "        )\n",
        "\n",
        "        histoty['validation_loss'].append(loss.item())\n",
        "    \n",
        "        \n",
        "    clear_output()\n",
        "    fig = plt.figure(figsize=(30, 15), dpi=80)\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    ax1.plot(histoty['train_loss'], label='Train')\n",
        "    ax1.set_xlabel('Iterations', fontsize=20)\n",
        "    ax1.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
        "    ax1.legend()\n",
        "    ax1.grid()\n",
        "    \n",
        "    ax2 = plt.subplot(3, 3, 4)\n",
        "    ax2.plot(histoty['validation_loss'], label='Validation')\n",
        "    ax2.set_xlabel('Iterations', fontsize=20)\n",
        "    ax2.set_ylabel(r'${L_1} + Perceptual$', fontsize=20)\n",
        "    ax2.legend()\n",
        "    ax2.grid()\n",
        "\n",
        "    for index, image in zip(\n",
        "        (2, 3, 5, 6),\n",
        "        (source_image, target_image, generated_image, regressed_depth.repeat(1, 3, 1,1))\n",
        "    ):\n",
        "        ax = plt.subplot(3, 3, index)\n",
        "        if index == 6:\n",
        "            im = ax.imshow(image.detach().cpu()[0].permute(1, 2, 0)[...,0])\n",
        "        else:\n",
        "            im = ax.imshow(renormalize_image(image.detach().cpu()[0]).permute(1, 2, 0))\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "c2486550",
        "execution": {
          "iopub.status.busy": "2022-05-13T19:13:10.685532Z",
          "iopub.execute_input": "2022-05-13T19:13:10.685791Z",
          "iopub.status.idle": "2022-05-13T19:15:16.299977Z",
          "shell.execute_reply.started": "2022-05-13T19:13:10.685762Z",
          "shell.execute_reply": "2022-05-13T19:15:16.299206Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize\n",
        "\n",
        "Goes along depth and generate new views"
      ],
      "metadata": {
        "id": "8559f1ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RTs = []\n",
        "for i in torch.linspace(0, 0.5, 40):\n",
        "    current_RT = torch.eye(4).unsqueeze(0)\n",
        "    current_RT[:, 2, 3] = i\n",
        "    RTs.append(current_RT.to(device))\n",
        "identity_matrx = torch.eye(4).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "c941802a",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:30:25.795951Z",
          "iopub.execute_input": "2022-05-11T11:30:25.796712Z",
          "iopub.status.idle": "2022-05-11T11:30:25.809771Z",
          "shell.execute_reply.started": "2022-05-11T11:30:25.796665Z",
          "shell.execute_reply": "2022-05-11T11:30:25.808263Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_instance_index = 12\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    images, cameras = validation_dataset[random_instance_index].values()\n",
        "    # Input values\n",
        "    input_img = images[0][None].cuda()\n",
        "\n",
        "    # Camera parameters\n",
        "    K = torch.from_numpy(cameras[0][\"K\"])[None].to(device)\n",
        "    K_inv = torch.from_numpy(cameras[0][\"Kinv\"])[None].to(device)\n",
        "    \n",
        "    spatial_features = model.spatial_feature_predictor(input_img)\n",
        "    regressed_depth = model.depth_regressor(input_img)\n",
        "    regressed_depth = (regressed_depth + 1.) / 2. # 0..1\n",
        "    regressed_depth = regressed_depth * (model.z_max - model.z_min) + model.z_min\n",
        "\n",
        "    new_images = []\n",
        "    for current_RT in RTs:\n",
        "        generated_features = model.point_cloud_renderer.forward_justpts(\n",
        "            spatial_features,\n",
        "            regressed_depth,\n",
        "            K,\n",
        "            K_inv,\n",
        "            identity_matrx,\n",
        "            identity_matrx,\n",
        "            current_RT,\n",
        "            None\n",
        "        )\n",
        "        generated_image = model.refinement_network(generated_features)\n",
        "        new_images.append(renormalize_image(generated_image.cpu()).clamp(0, 1).mul(255).to(torch.uint8))"
      ],
      "metadata": {
        "id": "d775b314",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:30:25.812024Z",
          "iopub.execute_input": "2022-05-11T11:30:25.812738Z",
          "iopub.status.idle": "2022-05-11T11:30:26.9993Z",
          "shell.execute_reply.started": "2022-05-11T11:30:25.812697Z",
          "shell.execute_reply": "2022-05-11T11:30:26.998508Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "to_img = lambda x: x[0].permute(1,2,0).numpy()\n",
        "import cv2\n",
        "# import torchvision\n",
        "video_name = 'video.mp4'\n",
        "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 20, (256, 256))\n",
        "\n",
        "for frame in new_images:\n",
        "    video.write(to_img(frame))\n",
        "\n",
        "# cv2.destroyAllWindows()\n",
        "video.release()\n",
        "# Alternatively\n",
        "# frames = torch.cat(new_images).permute(0, 2, 3, 1)\n",
        "# torchvision.io.write_video('video.mp4', frames, fps=20)\n"
      ],
      "metadata": {
        "id": "563bb460",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:30:27.000456Z",
          "iopub.execute_input": "2022-05-11T11:30:27.000702Z",
          "iopub.status.idle": "2022-05-11T11:30:27.27262Z",
          "shell.execute_reply.started": "2022-05-11T11:30:27.000671Z",
          "shell.execute_reply": "2022-05-11T11:30:27.271831Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(to_img(frame))"
      ],
      "metadata": {
        "id": "1dvuk1AYqyWN",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:30:27.274026Z",
          "iopub.execute_input": "2022-05-11T11:30:27.274295Z",
          "iopub.status.idle": "2022-05-11T11:30:27.535614Z",
          "shell.execute_reply.started": "2022-05-11T11:30:27.274259Z",
          "shell.execute_reply": "2022-05-11T11:30:27.534858Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(\"\"\"\n",
        "    <video width=\"256\" alt=\"test\" controls>\n",
        "        <source src=\"video.mp4\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "602c87f9",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:30:27.536934Z",
          "iopub.execute_input": "2022-05-11T11:30:27.537416Z",
          "iopub.status.idle": "2022-05-11T11:30:27.543845Z",
          "shell.execute_reply.started": "2022-05-11T11:30:27.537364Z",
          "shell.execute_reply": "2022-05-11T11:30:27.543145Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quality benchmark"
      ],
      "metadata": {
        "id": "822381d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide three video with your results for indices  101, 147 and 253. \n",
        "Also, put the result validation quality for your benchmark. Upload your View Synthesis module on google drive or yandex disk. We are not encourage you to train model as long as possible, because understand the limitation of the resources on Colab, that is why those data can help us to grade your work.\n",
        "\n",
        "| PSNR  | LPIPS  | \n",
        "|---|---|\n",
        "|   |   |\n",
        "|   |   |\n",
        "|   |   |"
      ],
      "metadata": {
        "id": "1c7aa5f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in [101, 147, 253]:\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        images, cameras = validation_dataset[index].values()\n",
        "        # Input values\n",
        "        input_img = images[0][None].cuda()\n",
        "        target_image = images[-1][None].cuda()\n",
        "\n",
        "        # Camera parameters\n",
        "        K = torch.from_numpy(cameras[0][\"K\"])[None].to(device)\n",
        "        K_inv = torch.from_numpy(cameras[0][\"Kinv\"])[None].to(device)\n",
        "\n",
        "        spatial_features = model.spatial_feature_predictor(input_img)\n",
        "        regressed_depth = model.depth_regressor(input_img)\n",
        "        regressed_depth = (regressed_depth + 1.) / 2. # 0..1\n",
        "        regressed_depth = regressed_depth * (model.z_max - model.z_min) + model.z_min\n",
        "        loss = 0\n",
        "        new_images = []\n",
        "        for current_RT in RTs:\n",
        "            generated_features = model.point_cloud_renderer.forward_justpts(\n",
        "                spatial_features,\n",
        "                regressed_depth,\n",
        "                K,\n",
        "                K_inv,\n",
        "                identity_matrx,\n",
        "                identity_matrx,\n",
        "                current_RT,\n",
        "                None\n",
        "            )\n",
        "            generated_image = model.refinement_network(generated_features)\n",
        "            new_images.append(renormalize_image(generated_image.cpu()).clamp(0, 1).mul(255).to(torch.uint8))\n",
        "            loss += l1_criterion(generated_image, target_image) \\\n",
        "            + 10 * perceptual_criterion(\n",
        "            renormalize_image(generated_image),\n",
        "            renormalize_image(target_image)\n",
        "        )\n",
        "        print(f\"For index {index} loss is {loss.item() / len(RTs)}\")\n",
        "        video_name = f'video_{index}.mp4'\n",
        "        video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 20, (256, 256))\n",
        "\n",
        "        for frame in new_images:\n",
        "            video.write(to_img(frame))\n",
        "\n",
        "        # cv2.destroyAllWindows()\n",
        "        video.release()"
      ],
      "metadata": {
        "id": "fcc06bee",
        "execution": {
          "iopub.status.busy": "2022-05-11T11:46:12.809736Z",
          "iopub.execute_input": "2022-05-11T11:46:12.810058Z",
          "iopub.status.idle": "2022-05-11T11:46:17.683115Z",
          "shell.execute_reply.started": "2022-05-11T11:46:12.810028Z",
          "shell.execute_reply": "2022-05-11T11:46:17.682279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For index 101 loss is 8.308778381347656\n",
        "\n",
        "For index 147 loss is 6.548649597167969\n",
        "\n",
        "For index 253 loss is 5.808666229248047"
      ],
      "metadata": {
        "id": "ZF6L0fuFY7OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(\"\"\"\n",
        "    <video width=\"256\" alt=\"test\" controls>\n",
        "        <source src=\"video_101.mp4\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-11T11:48:15.532762Z",
          "iopub.execute_input": "2022-05-11T11:48:15.533096Z",
          "iopub.status.idle": "2022-05-11T11:48:15.539716Z",
          "shell.execute_reply.started": "2022-05-11T11:48:15.533061Z",
          "shell.execute_reply": "2022-05-11T11:48:15.538949Z"
        },
        "trusted": true,
        "id": "hICl7Q46Y7OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(\"\"\"\n",
        "    <video width=\"256\" alt=\"test\" controls>\n",
        "        <source src=\"video_147.mp4\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-11T11:48:40.518553Z",
          "iopub.execute_input": "2022-05-11T11:48:40.518867Z",
          "iopub.status.idle": "2022-05-11T11:48:40.524689Z",
          "shell.execute_reply.started": "2022-05-11T11:48:40.518835Z",
          "shell.execute_reply": "2022-05-11T11:48:40.523991Z"
        },
        "trusted": true,
        "id": "6OG32auFY7OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(\"\"\"\n",
        "    <video width=\"256\" alt=\"test\" controls>\n",
        "        <source src=\"video_253.mp4\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "5WhgyaFmY7OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'checkpoint_100_epoch.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-11T11:55:02.307171Z",
          "iopub.execute_input": "2022-05-11T11:55:02.308072Z",
          "iopub.status.idle": "2022-05-11T11:55:02.412326Z",
          "shell.execute_reply.started": "2022-05-11T11:55:02.308031Z",
          "shell.execute_reply": "2022-05-11T11:55:02.411528Z"
        },
        "trusted": true,
        "id": "sMqtLkU-Y7OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'checkpoint_100_epoch.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-11T11:57:18.188621Z",
          "iopub.execute_input": "2022-05-11T11:57:18.188938Z",
          "iopub.status.idle": "2022-05-11T11:57:18.195188Z",
          "shell.execute_reply.started": "2022-05-11T11:57:18.188904Z",
          "shell.execute_reply": "2022-05-11T11:57:18.194222Z"
        },
        "trusted": true,
        "id": "Blexii7oY7OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3wQq178eY7OK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}